# === LLM CONFIGURATION ===
# Format: provider/model (e.g. openai/gpt-4o, anthropic/claude-3-5-sonnet, ollama/llama3)
LLM_MODEL=gemini/gemini-flash-lite-latest
LLM_API_KEY=your-key-here

# Only required for Ollama or LocalAI
# LLM_BASE_URL=http://localhost:11434

# === TOOL CONFIGURATION ===
# Use "API Read Access Token"
TMDB_API_KEY=your_tmdb_key_here
